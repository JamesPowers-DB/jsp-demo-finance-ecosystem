{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5228396c-10b0-4505-91c3-d31d5aad10b8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "\n",
    "from datetime import datetime, timedelta\n",
    "from dateutil.relativedelta import relativedelta\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DoubleType, BooleanType\n",
    "\n",
    "\n",
    "\n",
    "# ----------- DataFrames for Jobs --------------- #\n",
    "projects = spark.read.table('jsp_demo.fin.projects')\n",
    "\n",
    "\n",
    "# Legal Entities\n",
    "LEGAL_ENTITIES = {\n",
    "    'GB-US': {'name': 'GlobalBuild Americas Inc.', 'continent': 'Americas', 'currency': 'USD', 'revenue_split': 0.60},\n",
    "    'GB-EU': {'name': 'GlobalBuild EMEA Ltd.', 'continent': 'EMEA', 'currency': 'EUR', 'revenue_split': 0.40}\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e3dde6e6-3d1d-4eab-8e28-b4454ac5c34b",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1759511805545}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def generate_parts_master():\n",
    "    \"\"\"Generate parts/materials master data using PySpark\"\"\"\n",
    "\n",
    "    part_categories = {\n",
    "        'Concrete': ['Ready-Mix Concrete', 'Concrete Blocks', 'Cement Bags', 'Reinforcement Bars'],\n",
    "        'Steel': ['Structural Steel Beams', 'Steel Plates', 'Rebar', 'Steel Columns'],\n",
    "        'Lumber': ['2x4 Lumber', '2x6 Lumber', 'Plywood Sheets', 'Timber Beams'],\n",
    "        'Electrical': ['Wiring', 'Circuit Breakers', 'Conduit', 'Junction Boxes'],\n",
    "        'Plumbing': ['PVC Pipes', 'Copper Pipes', 'Valves', 'Fittings'],\n",
    "        'Glass': ['Window Panels', 'Glass Doors', 'Tempered Glass', 'Glazing'],\n",
    "        'Hardware': ['Bolts', 'Screws', 'Nails', 'Anchors'],\n",
    "        'Equipment': ['Crane Parts', 'Excavator Parts', 'Loader Parts', 'Tools']\n",
    "    }\n",
    "\n",
    "    uom_options = ['EA', 'LB', 'FT', 'SQ FT', 'CU YD', 'ROLL']\n",
    "\n",
    "    # Create flat list of (category, part_name) tuples with part IDs\n",
    "    parts_data = []\n",
    "    part_id = 1000\n",
    "    for category, items in part_categories.items():\n",
    "        for item in items:\n",
    "            parts_data.append((part_id, category, item))\n",
    "            part_id += 1\n",
    "\n",
    "    # Create DataFrame from the list\n",
    "    schema = StructType([\n",
    "        StructField('part_id_num', IntegerType(), True),\n",
    "        StructField('part_category', StringType(), True),\n",
    "        StructField('part_name', StringType(), True)\n",
    "    ])\n",
    "\n",
    "    parts_df = spark.createDataFrame(parts_data, schema)\n",
    "\n",
    "    # Use Spark functions to generate derived fields\n",
    "    result = parts_df.withColumn(\n",
    "        'part_id',\n",
    "        F.concat(F.lit('PART-'), F.col('part_id_num').cast('string'))\n",
    "    ).withColumn(\n",
    "        'part_number',\n",
    "        F.concat(F.lit('P'), F.col('part_id_num').cast('string'))\n",
    "    ).withColumn(\n",
    "        'unit_of_measure',\n",
    "        F.element_at(\n",
    "            F.array([F.lit(uom) for uom in uom_options]),\n",
    "            F.abs((F.hash('part_id_num') % len(uom_options)).cast('int')) + 1\n",
    "            )\n",
    "    ).withColumn(\n",
    "        'unit_cost',\n",
    "        F.round(10 + ((F.hash(F.concat(F.lit('cost'), 'part_id_num')) % 10000) / 10000.0 * 4990), 2)\n",
    "    ).withColumn(\n",
    "        'reorder_point',\n",
    "        ((F.hash(F.concat(F.lit('reorder'), 'part_id_num')) % 451).cast('int') + 50)\n",
    "    ).withColumn(\n",
    "        'is_active',\n",
    "        F.lit(True)\n",
    "    ).select(\n",
    "        'part_id', 'part_number', 'part_name', 'part_category',\n",
    "        'unit_of_measure', 'unit_cost', 'reorder_point', 'is_active'\n",
    "    )\n",
    "\n",
    "    return result\n",
    "\n",
    "parts_master = generate_parts_master()\n",
    "\n",
    "parts_master.display()\n",
    "# parts_master.writeTo('jsp_demo.fin.parts_master').createOrReplace()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b02af845-097a-4e5a-9018-bedd4d799767",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# 11. INVENTORY TRANSACTIONS\n",
    "# ============================================================================\n",
    "\n",
    "def generate_inventory_transactions(parts_master_df, projects_df):\n",
    "    \"\"\"Generate inventory movements (receipts and issues) using PySpark distributed compute\"\"\"\n",
    "\n",
    "    # Sample parts and projects\n",
    "    active_parts = parts_master_df.limit(20).select('part_id', 'part_number', 'part_name', 'unit_cost')\n",
    "    active_projects = projects_df.filter(F.col('status') == 'Active').limit(50).select(\n",
    "        'project_id', 'project_name', 'legal_entity_code'\n",
    "    )\n",
    "\n",
    "    legal_entities = list(LEGAL_ENTITIES.keys())\n",
    "\n",
    "    # Generate 2000 transaction records using range\n",
    "    num_transactions = 2000\n",
    "    transaction_base = spark.range(num_transactions).select(\n",
    "        F.col('id').cast('int').alias('txn_idx')\n",
    "    ).withColumn(\n",
    "        'transaction_id',\n",
    "        F.concat(F.lit('INV-'), (F.col('txn_idx') + 700000).cast('string'))\n",
    "    ).withColumn(\n",
    "        'transaction_days',\n",
    "        ((F.hash('txn_idx') % 1461).cast('int'))  # 1461 days from 2021-09-01 to 2025-08-31\n",
    "    ).withColumn(\n",
    "        'transaction_date',\n",
    "        F.date_add(F.lit('2021-09-01'), F.col('transaction_days'))\n",
    "    ).withColumn(\n",
    "        'is_receipt',\n",
    "        (F.hash(F.concat(F.lit('type'), 'txn_idx')) % 2) == 0\n",
    "    )\n",
    "\n",
    "    # Add part info using hash-based distribution\n",
    "    parts_with_idx = active_parts.withColumn('part_idx', F.monotonically_increasing_id())\n",
    "    part_count = active_parts.count()\n",
    "\n",
    "    transaction_with_parts = transaction_base.join(\n",
    "        parts_with_idx,\n",
    "        (F.hash(F.concat(F.lit('part'), 'txn_idx')) % 1000).cast('long') % F.lit(part_count) ==\n",
    "        (F.col('part_idx') % F.lit(part_count)),\n",
    "        'inner'\n",
    "    )\n",
    "\n",
    "    # Add project info for issues\n",
    "    projects_with_idx = active_projects.withColumn('proj_idx', F.monotonically_increasing_id())\n",
    "    project_count = active_projects.count()\n",
    "\n",
    "    transaction_with_projects = transaction_with_parts.join(\n",
    "        projects_with_idx,\n",
    "        (F.hash(F.concat(F.lit('proj'), 'txn_idx')) % 1000).cast('long') % F.lit(project_count) ==\n",
    "        (F.col('proj_idx') % F.lit(project_count)),\n",
    "        'left'\n",
    "    )\n",
    "\n",
    "    # Calculate quantities and create final records\n",
    "    result = transaction_with_projects.withColumn(\n",
    "        'transaction_type',\n",
    "        F.when(F.col('is_receipt'), F.lit('Receipt')).otherwise(F.lit('Issue'))\n",
    "    ).withColumn(\n",
    "        'quantity_base',\n",
    "        F.when(F.col('is_receipt'),\n",
    "               ((F.hash(F.concat(F.lit('qty_receipt'), 'txn_idx')) % 491).cast('int') + 10))\n",
    "        .otherwise(\n",
    "               ((F.hash(F.concat(F.lit('qty_issue'), 'txn_idx')) % 196).cast('int') + 5))\n",
    "    ).withColumn(\n",
    "        'quantity',\n",
    "        F.when(F.col('is_receipt'), F.col('quantity_base')).otherwise(-F.col('quantity_base'))\n",
    "    ).withColumn(\n",
    "        'total_value',\n",
    "        F.round(F.col('quantity') * F.col('unit_cost'), 2)\n",
    "    ).withColumn(\n",
    "        'legal_entity_code',\n",
    "        F.when(F.col('is_receipt'),\n",
    "               F.element_at(\n",
    "                    F.array([F.lit(le) for le in legal_entities]),\n",
    "                    F.abs((F.hash(F.concat(F.lit('le'), 'txn_idx')) % len(legal_entities)).cast('int'))+1\n",
    "                    ))\n",
    "        .otherwise(F.col('legal_entity_code'))\n",
    "    ).withColumn(\n",
    "        'project_id_final',\n",
    "        F.when(F.col('is_receipt'), F.lit(None).cast('string')).otherwise(F.col('project_id'))\n",
    "    ).withColumn(\n",
    "        'description',\n",
    "        F.when(F.col('is_receipt'),\n",
    "               F.concat(F.lit('Receipt of '), F.col('part_name')))\n",
    "        .otherwise(\n",
    "               F.concat(F.lit('Issue to '), F.col('project_name')))\n",
    "    ).select(\n",
    "        'transaction_id',\n",
    "        'transaction_type',\n",
    "        F.col('transaction_date').cast('string').alias('transaction_date'),\n",
    "        'part_id',\n",
    "        'part_number',\n",
    "        'part_name',\n",
    "        'quantity',\n",
    "        'unit_cost',\n",
    "        'total_value',\n",
    "        F.col('project_id_final').alias('project_id'),\n",
    "        'legal_entity_code',\n",
    "        'description'\n",
    "    )\n",
    "\n",
    "    return result\n",
    "\n",
    "    \n",
    "inventory_transactions = generate_inventory_transactions(parts_master, projects)\n",
    "inventory_transactions.writeTo('jsp_demo.fin.inventory_transactions').createOrReplace()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "3"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "04_generate_parts_data",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
