{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "33b66248-b032-4466-bc6d-77d7a177cfb2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, LongType, StringType, DoubleType\n",
    "from pyspark.sql.functions import col, sum as _sum, year, from_unixtime\n",
    "from faker import Faker\n",
    "import random\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# -------------------- KEY VARIABLES -------------------- #\n",
    "catalog = 'main'\n",
    "directory = f\"/Volumes/{catalog}/finance_lakehouse/data_gen_outputs\"\n",
    "output_path = f\"{directory}/inbound_contracts\"\n",
    "\n",
    "# Number of records to generate\n",
    "# 1000 contracts per year starting from 2023\n",
    "start_year = 2023\n",
    "end_year = datetime.now().year\n",
    "years = end_year - start_year + 1\n",
    "num_records = 1000 * years\n",
    "\n",
    "# Annual aggregate target: 1 Billion dollars\n",
    "annual_target = 1_000_000_000\n",
    "total_target_value = annual_target * years"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bed52e45-07e0-4da4-bd93-702d5a7f56fb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Generate synthetic data for the inbound_contracts table.\n",
    "This script uses PySpark and maintains referential integrity with suppliers and legal entities.\n",
    "\"\"\"\n",
    "\n",
    "# Initialize Faker\n",
    "fake = Faker()\n",
    "Faker.seed(42)\n",
    "random.seed(42)\n",
    "\n",
    "# Define schema\n",
    "schema = StructType([\n",
    "    StructField(\"contract_id\", LongType(), nullable=False),\n",
    "    StructField(\"contract_number\", StringType(), nullable=False),\n",
    "    StructField(\"supplier_id\", LongType(), nullable=False),\n",
    "    StructField(\"legal_entity_id\", LongType(), nullable=False),\n",
    "    StructField(\"contract_currency\", StringType(), nullable=False),\n",
    "    StructField(\"total_contract_value\", DoubleType(), nullable=False),\n",
    "    StructField(\"contract_start_date\", LongType(), nullable=False),\n",
    "    StructField(\"estimated_completion_date\", LongType(), nullable=False),\n",
    "    StructField(\"contract_status\", StringType(), nullable=False),\n",
    "    StructField(\"contract_type\", StringType(), nullable=False),\n",
    "    StructField(\"contract_description\", StringType(), nullable=False),\n",
    "    StructField(\"payment_terms\", StringType(), nullable=False)\n",
    "])\n",
    "\n",
    "# Enum values from schema\n",
    "CONTRACT_STATUS = [\"Active\", \"Pending\", \"Completed\", \"On-Hold\"]\n",
    "CONTRACT_TYPE = [\"Fixed-Price\", \"Time-and-Materials\", \"Cost-Plus\"]\n",
    "PAYMENT_TERMS = [\"Net-30\", \"Net-45\", \"Net-60\"]\n",
    "\n",
    "def generate_contract_value_pareto(target_avg, pareto_alpha=1.5):\n",
    "    \"\"\"\n",
    "    Generate contract values following Pareto principle (80/20 rule).\n",
    "    80% of total value should be in 20% of contracts.\n",
    "\n",
    "    Args:\n",
    "        target_avg: Target average contract value to maintain aggregate target\n",
    "        pareto_alpha: Shape parameter for Pareto distribution (higher = less extreme)\n",
    "    \"\"\"\n",
    "    # Pareto distribution - using alpha=1.5 for more controlled distribution\n",
    "    pareto_value = random.paretovariate(pareto_alpha)\n",
    "\n",
    "    # Scale to target average\n",
    "    # The mean of paretovariate(alpha) is alpha/(alpha-1) for alpha > 1\n",
    "    # For alpha=1.5, mean is 3.0\n",
    "    expected_mean = pareto_alpha / (pareto_alpha - 1)\n",
    "\n",
    "    # Scale and adjust to hit target average\n",
    "    value = (pareto_value / expected_mean) * target_avg\n",
    "\n",
    "    # Cap at reasonable maximum (100x average)\n",
    "    return round(min(value, target_avg * 100), 2)\n",
    "\n",
    "def generate_contract_description(contract_type):\n",
    "    \"\"\"Generate realistic contract description based on type.\"\"\"\n",
    "    descriptions = {\n",
    "        \"Fixed-Price\": [\n",
    "            \"Procurement of materials and equipment\",\n",
    "            \"Vendor services and support contract\",\n",
    "            \"Supply and delivery of goods\",\n",
    "            \"Facilities management services\"\n",
    "        ],\n",
    "        \"Time-and-Materials\": [\n",
    "            \"Consulting and advisory services procurement\",\n",
    "            \"IT and technical support services\",\n",
    "            \"Professional services agreement\",\n",
    "            \"Engineering services contract\"\n",
    "        ],\n",
    "        \"Cost-Plus\": [\n",
    "            \"Research and development services\",\n",
    "            \"Custom manufacturing procurement\",\n",
    "            \"Complex project services with variable scope\",\n",
    "            \"Long-term vendor services contract\"\n",
    "        ]\n",
    "    }\n",
    "    return random.choice(descriptions.get(contract_type, [\"General procurement contract\"]))\n",
    "\n",
    "# Generate data\n",
    "print(f\"Generating {num_records} records for inbound contracts from {start_year} to {end_year}...\")\n",
    "print(f\"Target total contract value: ${total_target_value:,.2f}\")\n",
    "print(f\"Target average contract value: ${total_target_value/num_records:,.2f}\")\n",
    "\n",
    "# Read reference data\n",
    "print(\"\\nReading reference data...\")\n",
    "suppliers_df = spark.read.json(f\"{directory}/suppliers\")\n",
    "coa_hierarchy_df = spark.read.json(f\"{directory}/coa_hierarchy\")\n",
    "\n",
    "# Collect reference IDs for sampling\n",
    "supplier_ids = [row.supplier_id for row in suppliers_df.select(\"supplier_id\").collect()]\n",
    "legal_entity_ids = [row.legal_entity_id for row in coa_hierarchy_df.select(\"legal_entity_id\").distinct().collect()]\n",
    "\n",
    "print(f\"Available supplier_ids: {len(supplier_ids)}\")\n",
    "print(f\"Available legal_entity_ids: {len(legal_entity_ids)}\")\n",
    "\n",
    "# Generate contract data\n",
    "data = []\n",
    "contract_id = 40000000  # Start with 8-digit ID\n",
    "used_contract_numbers = set()\n",
    "target_avg_value = total_target_value / num_records\n",
    "\n",
    "# Create weighted sampling for Pareto distribution on supplier_id\n",
    "# 20% of suppliers should get 80% of contracts\n",
    "random.shuffle(supplier_ids)\n",
    "pareto_cutoff = int(len(supplier_ids) * 0.2)\n",
    "high_frequency_suppliers = supplier_ids[:pareto_cutoff]\n",
    "low_frequency_suppliers = supplier_ids[pareto_cutoff:]\n",
    "\n",
    "# Contract distribution over years (starting from 2023)\n",
    "start_date_2023 = datetime(start_year, 1, 1)\n",
    "end_date_current = datetime.now()\n",
    "\n",
    "for i in range(num_records):\n",
    "    # Generate unique contract number\n",
    "    contract_number = f\"INB{str(contract_id)[-6:]}\"\n",
    "    while contract_number in used_contract_numbers:\n",
    "        contract_id += 1\n",
    "        contract_number = f\"INB{str(contract_id)[-6:]}\"\n",
    "    used_contract_numbers.add(contract_number)\n",
    "\n",
    "    # Each contract linked to only 1 supplier (per YAML instruction)\n",
    "    # Apply Pareto principle: 80% chance to pick from high-frequency suppliers\n",
    "    if random.random() < 0.8 and high_frequency_suppliers:\n",
    "        supplier_id = random.choice(high_frequency_suppliers)\n",
    "    else:\n",
    "        supplier_id = random.choice(low_frequency_suppliers if low_frequency_suppliers else supplier_ids)\n",
    "\n",
    "    # Select random legal entity\n",
    "    legal_entity_id = random.choice(legal_entity_ids)\n",
    "\n",
    "    # Contract type and related fields\n",
    "    contract_type = random.choice(CONTRACT_TYPE)\n",
    "\n",
    "    # Generate contract value using Pareto principle\n",
    "    total_contract_value = generate_contract_value_pareto(target_avg_value)\n",
    "\n",
    "    # Generate dates starting from 2023\n",
    "    total_days = (end_date_current - start_date_2023).days\n",
    "    days_offset = random.randint(0, total_days)\n",
    "    start_date = start_date_2023 + timedelta(days=days_offset)\n",
    "    duration_days = random.randint(30, 730)  # 1 month to 2 years\n",
    "    completion_date = start_date + timedelta(days=duration_days)\n",
    "\n",
    "    contract_start_date = int(start_date.timestamp())\n",
    "    estimated_completion_date = int(completion_date.timestamp())\n",
    "\n",
    "    # Determine status based on dates\n",
    "    current_date = int(datetime.now().timestamp())\n",
    "    if current_date > estimated_completion_date:\n",
    "        contract_status = \"Completed\"\n",
    "    elif current_date < contract_start_date:\n",
    "        contract_status = \"Pending\"\n",
    "    else:\n",
    "        contract_status = random.choices(\n",
    "            [\"Active\", \"On-Hold\"],\n",
    "            weights=[95, 5]  # More active contracts\n",
    "        )[0]\n",
    "\n",
    "    # Payment terms\n",
    "    payment_terms = random.choice(PAYMENT_TERMS)\n",
    "\n",
    "    data.append({\n",
    "        \"contract_id\": contract_id,\n",
    "        \"contract_number\": contract_number,\n",
    "        \"supplier_id\": supplier_id,\n",
    "        \"legal_entity_id\": legal_entity_id,\n",
    "        \"contract_currency\": \"USD\",\n",
    "        \"total_contract_value\": total_contract_value,\n",
    "        \"contract_start_date\": contract_start_date,\n",
    "        \"estimated_completion_date\": estimated_completion_date,\n",
    "        \"contract_status\": contract_status,\n",
    "        \"contract_type\": contract_type,\n",
    "        \"contract_description\": generate_contract_description(contract_type),\n",
    "        \"payment_terms\": payment_terms\n",
    "    })\n",
    "\n",
    "    contract_id += 1\n",
    "\n",
    "# Create DataFrame\n",
    "df = spark.createDataFrame(data, schema=schema)\n",
    "\n",
    "# Show sample\n",
    "print(\"\\nSample of generated data:\")\n",
    "df.show(10, truncate=False)\n",
    "\n",
    "# Statistics\n",
    "print(f\"\\nTotal records: {df.count()}\")\n",
    "print(f\"Unique contract IDs: {df.select('contract_id').distinct().count()}\")\n",
    "print(f\"Unique contract numbers: {df.select('contract_number').distinct().count()}\")\n",
    "print(f\"Unique supplier IDs: {df.select('supplier_id').distinct().count()}\")\n",
    "print(f\"Unique legal entity IDs: {df.select('legal_entity_id').distinct().count()}\")\n",
    "\n",
    "# Value statistics\n",
    "print(\"\\nContract value statistics:\")\n",
    "df.select(\"total_contract_value\").summary(\"count\", \"min\", \"max\", \"mean\", \"stddev\").show()\n",
    "\n",
    "# Calculate actual total value\n",
    "actual_total = df.agg({\"total_contract_value\": \"sum\"}).collect()[0][0]\n",
    "print(f\"\\nActual total contract value: ${actual_total:,.2f}\")\n",
    "print(f\"Target total contract value: ${total_target_value:,.2f}\")\n",
    "print(f\"Difference: ${actual_total - total_target_value:,.2f}\")\n",
    "\n",
    "# Annual contract amount by contract start date\n",
    "print(\"\\nAnnual contract value by start year:\")\n",
    "from pyspark.sql.functions import from_unixtime, year, sum as _sum, count\n",
    "df_with_year = df.withColumn(\"start_year\", year(from_unixtime(col(\"contract_start_date\"))))\n",
    "annual_summary = df_with_year.groupBy(\"start_year\").agg(\n",
    "    _sum(\"total_contract_value\").alias(\"total_value\"),\n",
    "    count(\"contract_id\").alias(\"contract_count\")\n",
    ").orderBy(\"start_year\")\n",
    "annual_summary.show()\n",
    "\n",
    "# Status distribution\n",
    "print(\"\\nContract status distribution:\")\n",
    "df.groupBy(\"contract_status\").count().show()\n",
    "\n",
    "# Type distribution\n",
    "print(\"\\nContract type distribution:\")\n",
    "df.groupBy(\"contract_type\").count().show()\n",
    "\n",
    "# Write to JSON\n",
    "print(f\"\\nWriting data to {output_path}...\")\n",
    "df.coalesce(1).write.mode(\"overwrite\").json(output_path)\n",
    "\n",
    "print(\"Data generation complete!\")\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": null
    }
   },
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "dependencies": [
     "-r '/Workspace/Users/james.powers@databricks.com/demo-finance-ecosystem/00.Data Generation/requirements.txt'"
    ],
    "environment_version": "3"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "05a_generate_inbound_contracts",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
