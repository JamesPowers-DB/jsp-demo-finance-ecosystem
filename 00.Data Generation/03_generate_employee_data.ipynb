{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a93914a3-cfcb-417d-aa96-8e2879cc4377",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "from pyspark.sql.types import StructType, StructField, LongType, StringType, DoubleType\n",
    "from pyspark.sql.functions import col\n",
    "from faker import Faker\n",
    "import random\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# -------------------- KEY VARIABLES -------------------- #\n",
    "catalog = 'fin_demo'\n",
    "directory = f\"/Volumes/{catalog}/fin/data_gen_outputs\"\n",
    "output_path = f\"{directory}/employees\"\n",
    "\n",
    "# Number of employees to generate\n",
    "num_records = 2000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6512a2a4-d336-499a-b2d1-46c258ffebcd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Generate synthetic data for the employees table.\n",
    "This script uses PySpark and Faker to create realistic employee data with linear growth from 2022.\n",
    "\"\"\"\n",
    "\n",
    "# Initialize Faker\n",
    "fake = Faker()\n",
    "Faker.seed(42)\n",
    "random.seed(42)\n",
    "\n",
    "\n",
    "# Define schema\n",
    "schema = StructType([\n",
    "    StructField(\"employee_id\", LongType(), nullable=False),\n",
    "    StructField(\"employee_name\", StringType(), nullable=False),\n",
    "    StructField(\"salary\", DoubleType(), nullable=False),\n",
    "    StructField(\"cost_center_id\", StringType(), nullable=False),\n",
    "    StructField(\"hire_date\", LongType(), nullable=False)\n",
    "])\n",
    "\n",
    "# Salary ranges by cost center\n",
    "cost_center_salary_ranges = {\n",
    "    \"Executive\": (150000, 500000),\n",
    "    \"Corporate\": (80000, 200000),\n",
    "    \"Sales Department\": (50000, 150000),\n",
    "    \"Marketing Department\": (55000, 140000),\n",
    "    \"Engineering\": (90000, 200000),\n",
    "    \"R&D\": (85000, 180000),\n",
    "    \"Finance\": (70000, 160000),\n",
    "    \"Manufacturing\": (40000, 100000)\n",
    "}\n",
    "\n",
    "# Generate data\n",
    "print(f\"Generating {num_records} employee records with linear growth from 2022...\")\n",
    "\n",
    "# Read COA hierarchy data to get cost center IDs\n",
    "coa_hierarchy_path = f\"{directory}/coa_hierarchy\"\n",
    "print(f\"Reading COA hierarchy data from {coa_hierarchy_path}...\")\n",
    "coa_df = spark.read.json(coa_hierarchy_path)\n",
    "\n",
    "# Get unique cost centers\n",
    "cost_centers = coa_df.select(\"cost_center_id\", \"cost_center_name\").distinct().collect()\n",
    "cost_center_list = [(row[\"cost_center_id\"], row[\"cost_center_name\"]) for row in cost_centers]\n",
    "\n",
    "print(f\"Found {len(cost_center_list)} cost centers\")\n",
    "\n",
    "# Start and end dates for hire_date distribution (2022-01-01 to present)\n",
    "start_date = datetime(2022, 1, 1)\n",
    "end_date = datetime.now()\n",
    "total_days = (end_date - start_date).days\n",
    "\n",
    "# Generate employee data with linear growth\n",
    "data = []\n",
    "used_names = set()\n",
    "employee_id = 50000000  # Start with 8-digit ID\n",
    "\n",
    "# Distribute employees over time with linear growth and slight variance\n",
    "for i in range(num_records):\n",
    "    # Linear growth: more recent hires\n",
    "    # Use quadratic distribution to simulate linear growth (more hires in recent years)\n",
    "    hire_progress = (random.random() ** 0.7)  # Bias towards more recent dates\n",
    "    days_offset = int(hire_progress * total_days)\n",
    "    hire_date = start_date + timedelta(days=days_offset)\n",
    "    hire_timestamp = int(hire_date.timestamp())\n",
    "\n",
    "    # Generate unique employee name\n",
    "    while True:\n",
    "        employee_name = fake.name()\n",
    "        if employee_name not in used_names:\n",
    "            used_names.add(employee_name)\n",
    "            break\n",
    "\n",
    "    # Select cost center (ensuring good distribution)\n",
    "    cost_center_id, cost_center_name = cost_center_list[i % len(cost_center_list)]\n",
    "\n",
    "    # Generate salary based on cost center\n",
    "    salary_range = cost_center_salary_ranges.get(cost_center_name, (50000, 100000))\n",
    "    salary = round(random.uniform(salary_range[0], salary_range[1]), 2)\n",
    "\n",
    "    data.append({\n",
    "        \"employee_id\": employee_id,\n",
    "        \"employee_name\": employee_name,\n",
    "        \"salary\": salary,\n",
    "        \"cost_center_id\": str(cost_center_id),\n",
    "        \"hire_date\": hire_timestamp\n",
    "    })\n",
    "\n",
    "    employee_id += 1\n",
    "\n",
    "# Create DataFrame\n",
    "df = spark.createDataFrame(data, schema=schema)\n",
    "\n",
    "# Show sample\n",
    "print(\"\\nSample of generated data:\")\n",
    "df.show(10, truncate=False)\n",
    "\n",
    "print(f\"\\nTotal records: {df.count()}\")\n",
    "print(f\"Unique employee IDs: {df.select('employee_id').distinct().count()}\")\n",
    "print(f\"Unique employee names: {df.select('employee_name').distinct().count()}\")\n",
    "print(f\"Unique cost centers: {df.select('cost_center_id').distinct().count()}\")\n",
    "\n",
    "# Show salary statistics by cost center\n",
    "print(\"\\nSalary statistics by cost center:\")\n",
    "df.groupBy(\"cost_center_id\").agg(\n",
    "    {\"salary\": \"avg\", \"salary\": \"min\", \"salary\": \"max\"}\n",
    ").show()\n",
    "\n",
    "# Write to JSON\n",
    "print(f\"\\nWriting data to {output_path}...\")\n",
    "df.coalesce(1).write.mode(\"overwrite\").json(output_path)\n",
    "\n",
    "print(\"Data generation complete!\")\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": null
    }
   },
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "dependencies": [
     "-r '/Workspace/Users/james.powers@databricks.com/jsp-demo-finance-ecosystem/00.Data Generation_v2/requirements.txt'"
    ],
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "03_generate_employee_data",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
