{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "95a2d1c6-0951-4ecd-9e50-cb4807e3294d",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1760492336828}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "SELECT DISTINCT account_name, account_number, MIN(gl_date), MAX(gl_date) FROM fin_demo.acct.fact_gl_entries GROUP BY ALL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ce65ea1e-28e6-47fc-940b-5f6bcf0fa3eb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "SELECT  \n",
    "  account_name, \n",
    "  account_number, \n",
    "  MIN(gl_date), \n",
    "  MAX(gl_date), \n",
    "  COUNT(*) FILTER (WHERE gl_date IS NULL) \n",
    "FROM fin_demo.acct.fact_gl_entries \n",
    "GROUP BY ALL ;\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "688c8f6b-de2c-41b7-938f-69ee84790592",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# TEST OVERALL DATASET BUILD\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"DATA QUALITY CHECKS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Check 1: Verify debits equal credits for each transaction\n",
    "print(\"\\n1. Checking if debits equal credits for each transaction...\")\n",
    "\n",
    "df_balance_check = spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        transaction_id,\n",
    "        transaction_type,\n",
    "        SUM(debit_amount) AS total_debits,\n",
    "        SUM(credit_amount) AS total_credits,\n",
    "        SUM(debit_amount) - SUM(credit_amount) AS variance\n",
    "    FROM fin_demo.acct.fact_gl_entries\n",
    "    GROUP BY transaction_id, transaction_type\n",
    "    HAVING ABS(SUM(debit_amount) - SUM(credit_amount)) > 0.01\n",
    "    ORDER BY variance DESC\n",
    "\"\"\")\n",
    "\n",
    "unbalanced_count = df_balance_check.count()\n",
    "if unbalanced_count == 0:\n",
    "    print(\"   ✓ PASS: All transactions are balanced!\")\n",
    "else:\n",
    "    print(f\"   ✗ FAIL: {unbalanced_count} transactions are not balanced\")\n",
    "    display(df_balance_check)\n",
    "\n",
    "# Check 2: Summary by account\n",
    "print(\"\\n2. Account Summary (Trial Balance)...\")\n",
    "\n",
    "df_account_summary = spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        account_number,\n",
    "        account_name,\n",
    "        COUNT(*) AS entry_count,\n",
    "        SUM(debit_amount) AS total_debits,\n",
    "        SUM(credit_amount) AS total_credits,\n",
    "        SUM(debit_amount) - SUM(credit_amount) AS net_balance\n",
    "    FROM fin_demo.acct.fact_gl_entries\n",
    "    GROUP BY account_number, account_name\n",
    "    ORDER BY account_number\n",
    "\"\"\")\n",
    "\n",
    "display(df_account_summary)\n",
    "\n",
    "# Check 3: Overall balance check\n",
    "print(\"\\n3. Overall Double-Entry Balance Check...\")\n",
    "\n",
    "df_overall = spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        SUM(debit_amount) AS total_debits,\n",
    "        SUM(credit_amount) AS total_credits,\n",
    "        SUM(debit_amount) - SUM(credit_amount) AS variance\n",
    "    FROM fin_demo.acct.fact_gl_entries\n",
    "\"\"\")\n",
    "\n",
    "overall_result = df_overall.collect()[0]\n",
    "if abs(overall_result['variance']) < 0.01:\n",
    "    print(\"   ✓ PASS: Overall debits equal credits!\")\n",
    "else:\n",
    "    print(f\"   ✗ FAIL: Overall variance = {overall_result['variance']}\")\n",
    "\n",
    "display(df_overall)\n",
    "\n",
    "# Check 4: Transaction type summary\n",
    "print(\"\\n4. Summary by Transaction Type...\")\n",
    "\n",
    "df_type_summary = spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        transaction_type,\n",
    "        COUNT(DISTINCT transaction_id) AS transaction_count,\n",
    "        COUNT(*) AS gl_entry_count,\n",
    "        SUM(debit_amount) AS total_debits,\n",
    "        SUM(credit_amount) AS total_credits\n",
    "    FROM fin_demo.acct.fact_gl_entries\n",
    "    GROUP BY transaction_type\n",
    "    ORDER BY transaction_type\n",
    "\"\"\")\n",
    "\n",
    "display(df_type_summary)\n",
    "\n",
    "# COMMAND ----------\n",
    "# ============================================================================\n",
    "# SAMPLE QUERIES FOR ANALYSIS\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"SAMPLE ANALYTICAL QUERIES\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Cash flow analysis\n",
    "print(\"\\n1. Cash Flow Summary...\")\n",
    "df_cash_flow = spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        DATE_TRUNC('month', gl_date) AS month,\n",
    "        SUM(debit_amount) AS cash_in,\n",
    "        SUM(credit_amount) AS cash_out,\n",
    "        SUM(debit_amount) - SUM(credit_amount) AS net_cash_flow\n",
    "    FROM fin_demo.acct.fact_gl_entries\n",
    "    WHERE account_number = 1000\n",
    "    GROUP BY DATE_TRUNC('month', gl_date)\n",
    "    ORDER BY month\n",
    "\"\"\")\n",
    "display(df_cash_flow)\n",
    "\n",
    "# AR aging\n",
    "print(\"\\n2. Accounts Receivable Summary...\")\n",
    "df_ar_summary = spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        SUM(debit_amount) AS ar_increases,\n",
    "        SUM(credit_amount) AS ar_decreases,\n",
    "        SUM(debit_amount) - SUM(credit_amount) AS outstanding_ar\n",
    "    FROM fin_demo.acct.fact_gl_entries\n",
    "    WHERE account_number = 1100\n",
    "\"\"\")\n",
    "display(df_ar_summary)\n",
    "\n",
    "# AP aging\n",
    "print(\"\\n3. Accounts Payable Summary...\")\n",
    "df_ap_summary = spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        SUM(credit_amount) AS ap_increases,\n",
    "        SUM(debit_amount) AS ap_decreases,\n",
    "        SUM(credit_amount) - SUM(debit_amount) AS outstanding_ap\n",
    "    FROM fin_demo.acct.fact_gl_entries\n",
    "    WHERE account_number = 2000\n",
    "\"\"\")\n",
    "display(df_ap_summary)\n",
    "\n",
    "# Revenue by category\n",
    "print(\"\\n4. Revenue by Category...\")\n",
    "df_revenue_category = spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        category,\n",
    "        SUM(credit_amount) AS total_revenue\n",
    "    FROM fin_demo.acct.fact_gl_entries\n",
    "    WHERE account_number = 4000\n",
    "    GROUP BY category\n",
    "    ORDER BY total_revenue DESC\n",
    "\"\"\")\n",
    "display(df_revenue_category)\n",
    "\n",
    "print(\"\\n✓ GL Entries generation complete!\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 6793160978955047,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "test_gl_entries",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
